{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20200844_Abbas_Mammadov_MAS350_HW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "$\\textbf{EM Algorithm for Mixture of Two Gaussian Distributions} $\\\n",
        "Input: $X = (x_{1}, x_{2}, ..., x_{N})$ is $N$-samples and $K$ is the number of Gaussian Distributions \\\n",
        "Output: MLE of $(X, parameters = (\\mu, \\sigma, \\gamma))$ and parameters themselves.\n",
        "\n",
        "Algorithm: \\\n",
        "1. Randomly initializing $\\mu, \\sigma, \\gamma$ with the length of $K$ and normalizing $\\gamma$ to make it sum add up to 1. \\\n",
        "2. Expectation step --> calculating weighted sums for $i=[1, 2, ..., N], k=[1, 2, ..., K]$\n",
        "$$p_{ik} = \\frac{\\gamma_{k}N(x_{i}|\\mu_{k}, \\sigma_{k})}{\\sum_{l=1}^{K} \\gamma_{l}N(x_{i}|\\mu_{l}, \\sigma_{l})}$$ \n",
        "3. Maximization step --> updating the parameters for $k=[1, 2, ..., K]$\n",
        "$$\\mu_{k} = \\frac{\\sum_{i=1}^{N} p_{ik}x_{i}}{\\sum_{i=1}^{N} p_{ik}}$$\n",
        "$$\\sigma_{k} = \\frac{\\sum_{i=1}^{N} p_{ik}(x_{i}-\\mu_{k})^2}{\\sum_{i=1}^{N} p_{ik}}$$\n",
        "$$\\gamma_{k} = \\frac{1}{N}\\sum_{i=1}^{N} p_{ik}$$\n",
        "4. Apply above (2), (3) repeatedly until observing the convergency of MLE by comparing previous and current values.\n",
        "\n",
        "\n",
        "Notes:\n",
        "1. I have also made helper functions to calculate the pdf of normal distributions and MLE in order to avoid very long lines of codes in main function.\n",
        "2. I used stopping value and compared pre and new states to conclude convergency, but we can also add max_iters (defaultly = 100) which will run the code with this depth. I added necessary code for that as well. Even though the estimated parameters will not be affected too much, one can also try to use max_iters version commented out in below codes.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_cuLt8A315RX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing numpy\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "wki9FwFQonTY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D2JZp6R5mlNx"
      },
      "outputs": [],
      "source": [
        "# Let's first define some function to calculate the pdf of Gaussian\n",
        "def normal_pdf(x, mu, sigma):\n",
        "    return (1.0 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-((x - mu) ** 2) / (2 * (sigma ** 2)))\n",
        "\n",
        "# ML-estimate for X=(x1, x2, ..., xn) and K-dim mean, variance, and weights\n",
        "def MLE(X, mu, sigma, gamma):\n",
        "    K = len(mu)\n",
        "    N = len(X)\n",
        "    MLEi = 0\n",
        "    MLE = 1\n",
        "    for i in range(N):\n",
        "        for k in range(K):\n",
        "            MLEi += gamma[k] * normal_pdf(X[i], mu[k], sigma[k])\n",
        "        MLE *= MLEi\n",
        "        MLEi = 0\n",
        "    return MLE\n",
        "\n",
        "# Let's solve the problem\n",
        "def EM_algorithm(X, K, stopping_criteria = 1e-6, max_iters = 100):\n",
        "    # Initialization\n",
        "    N = len(X)\n",
        "    mu = np.random.rand(K)\n",
        "    sigma = np.random.rand(K)\n",
        "    gamma = np.random.rand(K)\n",
        "    gamma = gamma / gamma.sum() # They are the weights and sum up to one (1)\n",
        "    # EM algorithm\n",
        "    MLE_pre = np.inf\n",
        "    MLE_new = MLE(X, mu, sigma, gamma)\n",
        "    while abs(MLE_pre - MLE_new) > stopping_criteria: # We can choose when to stop, not exactly zero but convergent to prev state\n",
        "    #for i in range(max_iters): # With max_iters --> another version to define termination\n",
        "        MLE_pre = MLE_new\n",
        "        # Expectation-step\n",
        "        p = np.zeros((N, K))\n",
        "        for i in range(N):\n",
        "            for k in range(K):\n",
        "                p[i][k] = gamma[k] * normal_pdf(X[i], mu[k], sigma[k])\n",
        "            p[i] = p[i] / p[i].sum()\n",
        "        # Maximization-step\n",
        "        for k in range(K):\n",
        "            pk = p[:, k].sum()\n",
        "            mu[k] = (p[:, k] * X).sum() / pk\n",
        "            sigma[k] = np.sqrt((p[:, k] * ((X - mu[k]) ** 2)).sum() / pk)\n",
        "            gamma[k] = pk / N\n",
        "        MLE_new = MLE(X, mu, sigma, gamma)\n",
        "    mle = MLE_new\n",
        "    params = (mu, sigma, gamma)\n",
        "    return mle, params\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To check the validity\n",
        "np.random.seed(1) # For reproducibility \n",
        "x = np.random.rand(100)\n",
        "print(EM_algorithm(x, 10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2uNHSgX1nHN",
        "outputId": "9e49750d-864d-4a8a-9130-560a8ad33b71"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6120106.7433827, (array([0.02645614, 0.55682318, 0.1739329 , 0.30038531, 0.91624781,\n",
            "       0.09945558, 0.41922496, 0.20463366, 0.14010361, 0.70795703]), array([0.01894344, 0.03711165, 0.00984908, 0.03607811, 0.04026135,\n",
            "       0.00936325, 0.01566649, 0.00565022, 0.00572324, 0.05991644]), array([0.08020811, 0.11348062, 0.03051238, 0.12211877, 0.16630179,\n",
            "       0.05992969, 0.11839452, 0.02850993, 0.0595889 , 0.2209553 ])))\n"
          ]
        }
      ]
    }
  ]
}